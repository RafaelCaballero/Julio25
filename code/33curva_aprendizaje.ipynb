{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RafaelCaballero/Julio25/blob/main/code/33curva_aprendizaje.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq5qwJRz_UpG"
      },
      "source": [
        "# Introducción a la ciencia de datos con Python\n",
        "Rafa Caballero\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Evaluación de modelos\n",
        "\n",
        "Con la validación cruzada hemos visto como:\n",
        "\n",
        "- Evaluar de forma fiable lo bueno que es un modelo (con `cross_val_score`)\n",
        "\n",
        "- Obtener los mejores valores para los hiperparámetros (con ayuda de `GridSearchCV` y `RandomizedSearchCV`)\n",
        "\n",
        "- Sin embargo quedan preguntas por resolver como  ¿Podemos mejorar los resultados con más datos? ¿tenemos overfitting?\n",
        "\n",
        "Las curvas de aprendizaje son un buen mecanismo para ver esto. La idea principal es:\n",
        "\n",
        "- El error sobre test es siempre mayor que el que se tiene sobre train, por eso train es una inferior del error\n",
        "\n",
        "- Tener en cuenta que cuando no hablamos de error sino de score es al reves, el train es una cota superior del error\n",
        "\n",
        "Veamos un ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rXQzdXy_UpI"
      },
      "outputs": [],
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqTjtnFi_UpI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import spacy\n",
        "\n",
        "#### Carga y preprocesado\n",
        "path = \"https://raw.githubusercontent.com/amankharwal/SMS-Spam-Detection/master/spam.csv\"\n",
        "#path = \"c:/hlocal/tdm/movimiento.csv\"\n",
        "df = pd.read_csv(path,encoding=\"latin1\").drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\n",
        "df[\"label\"]=0\n",
        "df.loc[df[\"class\"]==\"spam\",\"label\"] =1\n",
        "\n",
        "nlp = spacy.blank('en')\n",
        "stopwords = nlp.Defaults.stop_words\n",
        "\n",
        "l = [\" \".join([token.text.upper() for token in nlp(doc) if not token.is_stop and  token.is_alpha]) for doc in df.message]\n",
        "\n",
        "df[\"limpio\"] = l\n",
        "frases =df[\"limpio\"].values\n",
        "y = df[\"label\"].values\n",
        "# creating count vectorizer object\n",
        "countV = CountVectorizer()\n",
        "#tranforming values\n",
        "X = countV.fit_transform(frases)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d0pDFmx_UpJ"
      },
      "source": [
        "Utilizamos `cross_validate` en lugar de `cross_val_score`porque con el parámetro `train_score=True` nos devuelve los errores sobre el entrenamiento además de sobre el test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiBq6evn_UpJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold,cross_val_score,cross_validate\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "steps = [('BernoulliNB', BernoulliNB())]\n",
        "scorer = make_scorer(cohen_kappa_score)\n",
        "pipeline = Pipeline(steps=steps)\n",
        "cv = RepeatedStratifiedKFold(n_splits=20, n_repeats=10)\n",
        "cv_results = cross_validate(pipeline, X, y, scoring=scorer, cv=cv,return_train_score=True)\n",
        "cv_results.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyjnS4ct_UpJ"
      },
      "outputs": [],
      "source": [
        "cv_results[\"test_score\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfZ246FX_UpK"
      },
      "outputs": [],
      "source": [
        "cv_results[\"train_score\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoXaUQQV_UpK"
      },
      "source": [
        "Como vemos en general los valores sobre train serán mejores que sobre test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1yO9DFZ_UpK"
      },
      "outputs": [],
      "source": [
        "cv_results[\"test_score\"].mean(), cv_results[\"train_score\"].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEXmkia5_UpL"
      },
      "source": [
        "Sin embargo esta información todavía no es útil, obtendremos una visión más clara si tenemos estos datos para distintas proporciones de train y test. Hagámoslo primero manualmente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v03cg6Bp_UpL"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import make_scorer,f1_score, accuracy_score, recall_score,cohen_kappa_score\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "\n",
        "veces = 50\n",
        "gap=5\n",
        "total=100\n",
        "resultados = []\n",
        "for i in tqdm(range(gap,total-gap)):\n",
        "    test = 1-i/total\n",
        "    #print(\"test\",test)\n",
        "    resultados_train = []\n",
        "    resultados_test = []\n",
        "    for v in  range(veces): #tqdm(range(veces), desc =str(i)+\"-\"+str(round(test,3))):\n",
        "        # 2\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= test)\n",
        "        #print(len(X_train),len(X_test))\n",
        "        # 3\n",
        "        steps = [('MultinomialNB', MultinomialNB())]\n",
        "        metodo = Pipeline(steps=steps)\n",
        "        modelo = metodo.fit(X_train,y_train)\n",
        "\n",
        "        # 4\n",
        "        y_pred_test = modelo.predict(X_test)\n",
        "        kappa_test = cohen_kappa_score(y_test,y_pred_test)\n",
        "        resultados_test.append(kappa_test)\n",
        "\n",
        "        y_pred_train = modelo.predict(X_train)\n",
        "        kappa_train =cohen_kappa_score(y_train,y_pred_train)\n",
        "        resultados_train.append(kappa_train)\n",
        "        #print(mae_train,mae_test)\n",
        "\n",
        "    #### acumulamos resultados\n",
        "    valores = (test, np.array(resultados_train).mean(), np.array(resultados_test).mean())\n",
        "    #print(valores)\n",
        "    resultados.append(valores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOVG0MTC_UpM"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15, 5))\n",
        "y_train = [b for a,b,c in resultados ]\n",
        "y_test = [c for a,b,c in resultados ]\n",
        "x = [1-a for a,b,c in resultados ]\n",
        "\n",
        "ax.scatter(x,y_train,color=\"blue\")\n",
        "ax.plot(x,y_train,label=\"train\",color=\"blue\")\n",
        "ax.scatter(x,y_test,color=\"orange\")\n",
        "ax.plot(x,y_test,label=\"test\",color=\"orange\")\n",
        "plt.xlabel(\"Proporción train\")\n",
        "plt.ylabel(\"Kappa\")\n",
        "plt.legend()\n",
        "plt.title(\"Curva de aprendizaje\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lN5PyqRm_UpM"
      },
      "outputs": [],
      "source": [
        "min(y_train),max(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZgSsve5_UpM"
      },
      "source": [
        "Vemos que las curvas son prácticamente estables, hay poca diferencia. El test\n",
        "podría mejorar con más datos pero nunca por encima de 0.94 (posiblemente menos alrededor de 0.92). Es un sistema que aprende muy rápido y se estabiliza con pocos datos, luego la mejora es muy lenta\n",
        "\n",
        "**Ejercicio** Probar a añadir un paso RandomOverSampler ¿cambia algo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l9k7jI3_UpN"
      },
      "source": [
        "En lugar de hacerlo a mano podemos hacerlo con el método `learning_curve`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50wYCrtZ_UpN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "veces = 50\n",
        "gap=5\n",
        "total=100\n",
        "entrena = [i/total for i in range(gap,total-gap)]\n",
        "cv = RepeatedStratifiedKFold(n_splits=20, n_repeats=10)\n",
        "metodo = MultinomialNB()\n",
        "train_size_abs, train_scores, test_scores = learning_curve( metodo, X, y,train_sizes=entrena,\n",
        "                                                           cv=cv,verbose=2,n_jobs=-1,scoring=scorer,shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK3iJbcw_UpN"
      },
      "source": [
        "Ahora representamos ambos valores, los de entrenamiento y los de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWwrmRiY_UpN"
      },
      "outputs": [],
      "source": [
        "len(train_scores), len(train_scores[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJKDB0ai_UpN"
      },
      "outputs": [],
      "source": [
        "trainS = [s.mean() for s in train_scores]\n",
        "testS = [s.mean() for s in test_scores]\n",
        "coordX = list(range(len(trainS)))\n",
        "fig, ax = plt.subplots(figsize=(15, 5))\n",
        "ax.scatter(x,y_train,color=\"blue\")\n",
        "ax.plot(x,y_train,label=\"train\",color=\"blue\")\n",
        "ax.scatter(x,y_test,color=\"orange\")\n",
        "ax.plot(x,y_test,label=\"test\",color=\"orange\")\n",
        "plt.xlabel(\"Proporción train\")\n",
        "plt.ylabel(\"Kappa\")\n",
        "plt.legend()\n",
        "plt.title(\"Curva de aprendizaje\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4NIRdEE_UpO"
      },
      "source": [
        "Hay incluso un método que calcula los valores y muestra la gráfica, todo junto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVKKLrFq_UpO"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import LearningCurveDisplay\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 5))\n",
        "common_params = {\n",
        "    \"X\": X,\n",
        "    \"y\": y,\n",
        "    \"train_sizes\": np.linspace(0.05, 0.95, 100),\n",
        "    \"cv\": cv,\n",
        "    \"score_type\": \"both\",\n",
        "    \"n_jobs\": -1,\n",
        "    \"line_kw\": {\"marker\": \"o\"},\n",
        "    \"std_display_style\": \"fill_between\",\n",
        "    \"score_name\": \"kappa\",\n",
        "    \"scoring\":scorer\n",
        "}\n",
        "\n",
        "LearningCurveDisplay.from_estimator(metodo, **common_params, ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ErjaSzT_UpO"
      },
      "source": [
        "Veamos otro ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCELYqIU_UpO"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/RafaelCaballero/tdm/master/datos/usa2020/twords.csv\"\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "df = df.drop(columns=[\"id\"])\n",
        "label = \"label\"\n",
        "XColumns = [c for c in df.columns if c!=label]\n",
        "yColumn = label\n",
        "X = df[XColumns]\n",
        "y = df[yColumn]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjPhn-qL_UpO"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15, 5))\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1)\n",
        "common_params = {\n",
        "    \"X\": X,\n",
        "    \"y\": y,\n",
        "    \"train_sizes\": np.linspace(0.1, 0.95, 100),\n",
        "    \"cv\": cv,\n",
        "    \"score_type\": \"both\",\n",
        "    \"n_jobs\": -1,\n",
        "    \"line_kw\": {\"marker\": \"o\"},\n",
        "    \"std_display_style\": \"fill_between\",\n",
        "    \"score_name\": \"kappa\",\n",
        "    \"scoring\":scorer\n",
        "}\n",
        "\n",
        "LearningCurveDisplay.from_estimator(MultinomialNB(), **common_params, ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bC4_UPC5_UpP"
      },
      "source": [
        "¿Qué consecuencias se pueden sacar? ¿Vendría bien tener más datos para entrenar?\n",
        "\n",
        "Aún un tercer ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYrwRZt__UpP"
      },
      "outputs": [],
      "source": [
        "file = \"https://raw.githubusercontent.com/RafaelCaballero/tdm/master/datos/bus.csv\"\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "df = pd.read_csv(file)\n",
        "df[\"label\"] = 1\n",
        "df.loc[df.I8<580, \"label\"] = 0\n",
        "df = df.drop(columns=[\"I8\"])\n",
        "yColumn = \"label\"\n",
        "XColumns = [c for c in df.columns if c!=yColumn]\n",
        "X = df[XColumns]\n",
        "y = df[yColumn]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhUP6XZ1_UpP"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15, 5))\n",
        "cv = RepeatedStratifiedKFold(n_splits=20, n_repeats=5)\n",
        "common_params = {\n",
        "    \"X\": X,\n",
        "    \"y\": y,\n",
        "    \"train_sizes\": np.linspace(0.1, 1.0, 100),\n",
        "    \"cv\": cv,\n",
        "    \"score_type\": \"both\",\n",
        "    \"n_jobs\": -1,\n",
        "    \"line_kw\": {\"marker\": \"o\"},\n",
        "    \"std_display_style\": \"fill_between\",\n",
        "    \"score_name\": \"kappa\",\n",
        "    \"scoring\":scorer\n",
        "}\n",
        "\n",
        "LearningCurveDisplay.from_estimator(LogisticRegression(), **common_params, ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URO6hBf3_UpP"
      },
      "source": [
        "¿En este caso interesa tener más datos de entrenamiento?\n",
        "\n",
        "Es interesante ver cómo cambia la curva al cambiar el método"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wctxeFqN_UpP"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "fig, ax = plt.subplots(figsize=(15, 5))\n",
        "LearningCurveDisplay.from_estimator(GaussianNB(), **common_params, ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsbwJKHG_UpQ"
      },
      "source": [
        "En el caso del Titanic no podemos lograr más datos de entrenamiento, pero aun así podemos ver la curva y qué sucede:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzcCp0gX_UpQ"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/RafaelCaballero/tdm/master/datos/titanicyesno.csv\"\n",
        "import pandas as pd\n",
        "df = pd.read_csv(url)\n",
        "columnas = [\"survived\", \"pclass\", \"sex\", \"age\", \"sibsp\", \"parch\", \"fare\",\"embarked\"]\n",
        "df2 = df[columnas]\n",
        "df2 = df2.dropna()\n",
        "df2[\"survived\"] = df2.survived.replace(('yes', 'no'), (1, 0))\n",
        "df2[\"sex\"] = df2.sex.replace(('female', 'male'), (1, 0))\n",
        "df = pd.get_dummies(df2, columns=[\"embarked\"])\n",
        "\n",
        "yColumn = \"survived\"\n",
        "XColumns = [c for c in df.columns if c!=yColumn]\n",
        "X= df[XColumns]\n",
        "y=df[yColumn]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eft3KsFN_UpQ"
      },
      "source": [
        "Probar con regresión logística y con GaussianNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3maGlp5z_UpR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpKdXFGY_UpR"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYXccQyF_UpR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}